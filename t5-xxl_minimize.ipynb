{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198f8a5-86ab-4908-85c3-4c4dea8f0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/lyutyuh/ASP.git asp\n",
    "#cd asp\n",
    "#export ASP=$PWD # setting environment variable\n",
    "#conda env create -f environment.yml\n",
    "#wget https://polybox.ethz.ch/index.php/s/bFf8vJBonIT7sr8/download -O ./data/conll03_ner.zip\n",
    "#unzip ./data/conll03_ner.zip -d ./data\n",
    "#rm -rvf ./data/conll03_ner.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b547103-f760-4f00-8b52-8ed84641a19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import tempfile\n",
    "import subprocess\n",
    "import collections\n",
    "\n",
    "from collections import defaultdict\n",
    "from unittest import result\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "\n",
    "MENTION_START = '<m>'\n",
    "MENTION_END   = '</m>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269f636-8d0c-4c35-8f85-e6b345042463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target_sentences(\n",
    "    mentions, sentence, \n",
    "    inv_subtoken_map, subtoken_map,\n",
    "    entity_labels, m_special_start, m_special_end\n",
    "):\n",
    "    if len(mentions) > 0:\n",
    "        m_types = [x['type'] for x in mentions]\n",
    "        m_startings = [x['start'] for x in mentions]\n",
    "        m_endings = [x['end'] for x in mentions]\n",
    "    else:\n",
    "        m_types, m_startings, m_endings = [], [], []\n",
    "\n",
    "    sorted_pos = sorted(\n",
    "        [(inv_subtoken_map[x][0], m_special_end, entity_labels[t], ind) for ind,(x,t) in enumerate(zip(m_endings,m_types))] + \\\n",
    "        [(inv_subtoken_map[x][0], m_special_start, t, ind) for ind,(x,t) in enumerate(zip(m_startings,m_types))],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    target_sentence = copy.deepcopy(sentence)\n",
    "    ent_indices = [-1 for i in range(len(sentence))]\n",
    "    ent_type_sequence = [-1 for i in range(len(sentence))]\n",
    "    target_subtoken_map = copy.deepcopy(subtoken_map)\n",
    "\n",
    "    end_to_index_in_target = {}\n",
    "\n",
    "    for x in sorted_pos:\n",
    "        target_sentence.insert(x[0], x[1]) # insert end or start\n",
    "        ent_indices.insert(x[0], x[3]) # insert pairing bracket index for entity\n",
    "\n",
    "        if x[1] == m_special_end: # insert entity type\n",
    "            ent_type_sequence.insert(x[0], x[2])\n",
    "        else:\n",
    "            ent_type_sequence.insert(x[0], -1)\n",
    "\n",
    "        for k in end_to_index_in_target: # map index in src to index in target\n",
    "            # plus 1 for every special token inserted\n",
    "            end_to_index_in_target[k] += 1\n",
    "        end_to_index_in_target[x[0]] = x[0]\n",
    "\n",
    "        if x[1] == m_special_end:\n",
    "            target_subtoken_map.insert(x[0], subtoken_map[x[0]-1])\n",
    "        elif x[1] == m_special_start:\n",
    "            target_subtoken_map.insert(x[0], subtoken_map[x[0]+1])\n",
    "\n",
    "    return (\n",
    "        target_sentence,\n",
    "        ent_indices,\n",
    "        ent_type_sequence,\n",
    "        end_to_index_in_target,\n",
    "        target_subtoken_map\n",
    "    )\n",
    "\n",
    "\n",
    "def is_punctuation(c):\n",
    "    if (\n",
    "        c in {\".\", \",\", \"?\", \"!\", \";\", \n",
    "        \":\", \"'s\", \"'m\", \"'ve\", \"n't\", \"'ll\",\n",
    "        \")\", \"}\", \"]\"}\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_special(c):\n",
    "    if (\n",
    "        c in {\"<pad>\", \"</s>\", \"<unk>\"}\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_doc_subtokens(doc):\n",
    "    processed_doc, subtoken_map = [], []\n",
    "    word_idx = -1\n",
    "    first_token_in_doc = True\n",
    "    for word in doc:\n",
    "        word_idx += 1\n",
    "        if first_token_in_doc:\n",
    "            # insert prefix\n",
    "            prefix_text = tokenizer.tokenize(\"named entity recognition:\")\n",
    "            for sidx, subtoken in enumerate(prefix_text):\n",
    "                processed_doc.append(subtoken)\n",
    "                subtoken_map.append(word_idx)\n",
    "\n",
    "        subtokens = get_subtokens(word)\n",
    "        for sidx, subtoken in enumerate(subtokens):\n",
    "            processed_doc.append(subtoken)\n",
    "            subtoken_map.append(word_idx)\n",
    "\n",
    "        first_token_in_doc = False\n",
    "    processed_doc.append(\"</s>\")\n",
    "    subtoken_map.append(word_idx+1)\n",
    "\n",
    "    return processed_doc, subtoken_map\n",
    "\n",
    "\n",
    "def minimize_partition(\n",
    "    name, entity_labels, stats,\n",
    "    tokenizer, input_dir, output_dir\n",
    "):\n",
    "    if \"conll03\" in input_dir:\n",
    "        input_path = f\"{input_dir}/conll03_{name}.json\"\n",
    "\n",
    "    output_path = f\"{output_dir}/{name}.t5-small.jsonlines\"\n",
    "\n",
    "    print(\"Minimizing {}\".format(input_path))\n",
    "    processed_dataset = []\n",
    "    max_target_len, max_input_len = 0, 0\n",
    "\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        instances = json.load(input_file)\n",
    "\n",
    "        for ins_id, instance in enumerate(instances):\n",
    "            processed, subtoken_map = [], []\n",
    "            inv_subtoken_map = {}\n",
    "\n",
    "            tokens = instance['tokens']\n",
    "            entities = instance['entities']\n",
    "            extended = instance['extended']\n",
    "\n",
    "            word_idx = -1\n",
    "            for word in tokens:\n",
    "                # no prefix inserted here\n",
    "                word_idx += 1\n",
    "\n",
    "                subtokens = get_subtokens(word)\n",
    "                inv_subtoken_map[word_idx] = (len(processed), len(processed)+len(subtokens))\n",
    "\n",
    "                for sidx, subtoken in enumerate(subtokens):\n",
    "                    processed.append(subtoken)\n",
    "                    subtoken_map.append(word_idx)\n",
    "\n",
    "            inv_subtoken_map[word_idx+1] = (len(processed), len(processed)+1)\n",
    "            processed.append(\"</s>\")\n",
    "            subtoken_map.append(word_idx+1)\n",
    "            \n",
    "            target_sentence, ent_indices, ent_type_sequence, end_to_index_in_target, target_subtoken_map = get_target_sentences(\n",
    "                entities, processed, \n",
    "                inv_subtoken_map, subtoken_map,\n",
    "                entity_labels, MENTION_START, MENTION_END\n",
    "            )\n",
    "            if \"sentence_idx\" in instance:\n",
    "                def clamp(x, l, u):\n",
    "                    return max(min(x, u), l)\n",
    "                sentence_idx = [instance['sentence_idx'][clamp(x, 0, len(instance['sentence_idx'])-1)] for x in subtoken_map]\n",
    "                target_sentence_idx = [instance['sentence_idx'][clamp(x, 0, len(instance['sentence_idx'])-1)] for x in target_subtoken_map]\n",
    "            \n",
    "            max_target_len = max(max_target_len, len(target_sentence))\n",
    "            input_sentence, input_subtoken_map = get_doc_subtokens(doc=extended)\n",
    "            res = {\n",
    "                \"doc_id\": name+\"_\"+str(ins_id),\n",
    "                \"sentence\": processed, \n",
    "                # sentence is for copy mechanism, might be different from \n",
    "                # input_sentence which is for encoding only\n",
    "                \"input_sentence\": input_sentence,\n",
    "                \"subtoken_map\": subtoken_map,\n",
    "                \"target_sentence\": target_sentence,\n",
    "                \"ent_type_sequence\": ent_type_sequence,\n",
    "                \"ent_indices\": ent_indices\n",
    "            }\n",
    "            max_input_len = max(max_input_len, len(res['input_sentence']))\n",
    "            if \"sentence_idx\" in instance:\n",
    "                res['sentence_idx'] = sentence_idx\n",
    "                res['target_sentence_idx'] = target_sentence_idx\n",
    "                assert len(res['sentence_idx']) == len(res['sentence'])\n",
    "                assert len(res['target_sentence_idx']) == len(res['target_sentence'])\n",
    "            processed_dataset.append(res)\n",
    "\n",
    "    with open(output_path, \"w\") as output_file:\n",
    "        json.dump(processed_dataset, output_file)\n",
    "\n",
    "    print(\"Maximum input sequence length: {}\".format(max_input_len))\n",
    "    print(\"Maximum target sequence length: {}\".format(max_target_len))\n",
    "    print(\"Wrote {} sentences to {}\".format(len(processed_dataset), output_path))\n",
    "\n",
    "def normalize_word(word, language):\n",
    "    if language == \"arabic\":\n",
    "        word = word[:word.find(\"#\")]\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    elif word == \"''\" or word == \"``\":  # <unk> otherwise\n",
    "        return \"\\\"\"\n",
    "    elif word == \"`\":  # <unk> otherwise\n",
    "        return \"\\'\"\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_subtokens(word):\n",
    "    word = normalize_word(word, \"english\")\n",
    "    if word == \"(\" or word == \"[\":\n",
    "        subtokens = tokenizer.tokenize(word)\n",
    "    elif word in [\")\", \"]\", \"\\'\"]:\n",
    "        subtokens = tokenizer.tokenize(word)[1:]  # skipping '_'\n",
    "    elif is_punctuation(word):\n",
    "        subtokens = tokenizer.tokenize(word)[1:]  # skipping '_'\n",
    "    else:\n",
    "        subtokens = tokenizer.tokenize(word)\n",
    "    return subtokens\n",
    "\n",
    "\n",
    "def minimize_language(\n",
    "    entity_labels, stats,\n",
    "    input_dir, output_dir\n",
    "):\n",
    "    # including typed markers\n",
    "    tokenizer.add_tokens(MENTION_START)\n",
    "    tokenizer.add_tokens(MENTION_END)\n",
    "\n",
    "    if \"conll03\" in input_dir:\n",
    "        for name in [\"dev\", \"test\", \"train\"]:\n",
    "            minimize_partition(\n",
    "                name, entity_labels, stats, \n",
    "                tokenizer, input_dir, output_dir\n",
    "            )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545849b2-1e80-49cf-9f59-455dc68a5e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = \"./data/conll03_ner\"\n",
    "output_dir = \"./data/conll03_ner\"\n",
    "\n",
    "if \"conll03\" in input_dir:\n",
    "    typefile = f\"{input_dir}/conll03_types.json\"\n",
    "\n",
    "with open(typefile) as input_file:\n",
    "    labels = json.load(input_file)\n",
    "entity_labels = {}\n",
    "\n",
    "for k in labels['entities'].keys():\n",
    "    entity_labels[k] = len(entity_labels)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "stats = defaultdict(int)\n",
    "minimize_language(\n",
    "    entity_labels, stats, \n",
    "    input_dir, output_dir\n",
    ")\n",
    "print(\"stats:\", stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
